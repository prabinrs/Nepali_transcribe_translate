{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOavjQmnn5fO//qWJ1mhxnF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabinrs/Nepali_transcribe_translate/blob/main/notebook/translation_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-qdOzBTUHQl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from llm_service import translate_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration (replace with your actual keys/URLs or environment variables) ---\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"YOUR_GEMINI_API_KEY_HERE\")\n",
        "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
        "OLLAMA_MODEL_NAME = os.getenv(\"OLLAMA_MODEL_NAME\", \"llama2\") # Make sure this model is pulled in Ollama\n",
        "VLLM_BASE_URL = os.getenv(\"VLLM_BASE_URL\", \"http://localhost:8000\")"
      ],
      "metadata": {
        "id": "FcwOpYpDURlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Sample Nepali Text ---\n",
        "nepali_text = \"नमस्ते, तपाईंलाई कस्तो छ? मलाई आशा छ कि यो अनुवाद राम्रो छ।\" # Nepali: Hello, how are you? I hope this translation is good.\n",
        "\n",
        "print(f\"Original Nepali Text: {nepali_text}\\n\")\n"
      ],
      "metadata": {
        "id": "nVUmfjSIUWNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test Translation with Gemini ---\n",
        "try:\n",
        "    print(\"--- Translating with Gemini ---\")\n",
        "    translated_gemini = translate_text(\n",
        "        text_to_translate=nepali_text,\n",
        "        target_language=\"English\",\n",
        "        model_provider=\"Gemini\",\n",
        "        gemini_api_key=GEMINI_API_KEY\n",
        "    )\n",
        "    print(f\"Gemini Translation: {translated_gemini}\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error translating with Gemini: {e}\\n\")\n",
        "\n",
        "\n",
        "# --- Test Translation with Ollama ---\n",
        "# Ensure Ollama server is running (e.g., `ollama serve`) and the model is pulled (`ollama pull llama2`)\n",
        "try:\n",
        "    print(\"--- Translating with Ollama ---\")\n",
        "    translated_ollama = translate_text(\n",
        "        text_to_translate=nepali_text,\n",
        "        target_language=\"English\",\n",
        "        model_provider=\"Ollama\",\n",
        "        ollama_base_url=OLLAMA_BASE_URL,\n",
        "        ollama_model_name=OLLAMA_MODEL_NAME\n",
        "    )\n",
        "    print(f\"Ollama Translation: {translated_ollama}\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error translating with Ollama: {e}\\n\")\n",
        "\n",
        "\n",
        "# --- Test Translation with vLLM ---\n",
        "# Ensure vLLM server is running (e.g., `python -m vllm.entrypoints.api_server ...`)\n",
        "try:\n",
        "    print(\"--- Translating with vLLM ---\")\n",
        "    translated_vllm = translate_text(\n",
        "        text_to_translate=nepali_text,\n",
        "        target_language=\"English\",\n",
        "        model_provider=\"vLLM\",\n",
        "        vllm_base_url=VLLM_BASE_URL\n",
        "    )\n",
        "    print(f\"vLLM Translation: {translated_vllm}\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error translating with vLLM: {e}\\n\")"
      ],
      "metadata": {
        "id": "9RBSRg86UaWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CtK-qrgIUVso"
      }
    }
  ]
}